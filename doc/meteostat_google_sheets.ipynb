{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b72632f",
   "metadata": {},
   "source": [
    "# üå¶Ô∏è Traitement des donn√©es m√©t√©o avec Meteostat et Google Sheets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87debe9",
   "metadata": {},
   "source": [
    "## 1.  Importation des biblioth√®ques n√©cessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2553b096",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from meteostat import Point, Daily\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9583cec4",
   "metadata": {},
   "source": [
    "## 2.  Initialisation des chemins et coordonn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e472b8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coordonn√©es de Cape Town\n",
    "cape_town = Point(-33.9249, 18.4241)\n",
    "\n",
    "# Chemins personnalis√©s - √† adapter selon votre environnement\n",
    "base_path = os.getcwd()\n",
    "data_brut_path = os.path.join(base_path, 'data/data_brut/cape_town-20-25')\n",
    "data_propre_path = os.path.join(base_path, 'data/data_propre/cape_town-20-25')\n",
    "data_final_csv = os.path.join(base_path, 'data/data_pret/cape_town.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b6a9f1",
   "metadata": {},
   "source": [
    "## 3.  R√©cup√©ration des donn√©es m√©t√©o de Meteostat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492f9aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_save_history(start_date: str = \"2020-01-01\", output_dir: str = data_brut_path):\n",
    "    try:\n",
    "        start = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "        end = datetime.now()\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        data = Daily(cape_town, start, end).fetch()\n",
    "        if data.empty:\n",
    "            print(\" Aucune donn√©e r√©cup√©r√©e.\")\n",
    "            return\n",
    "\n",
    "        for date, row in data.iterrows():\n",
    "            filename = f\"cape_town_{date.strftime('%Y-%m-%d')}.json\"\n",
    "            path = os.path.join(output_dir, filename)\n",
    "            row_cleaned = row.where(pd.notnull(row), None)\n",
    "            weather_info = {\n",
    "                \"city\": \"Cape Town\",\n",
    "                \"date\": date.strftime(\"%Y-%m-%d\"),\n",
    "                \"temperature_avg\": row_cleaned.get(\"tavg\"),\n",
    "                \"temperature_min\": row_cleaned.get(\"tmin\"),\n",
    "                \"temperature_max\": row_cleaned.get(\"tmax\"),\n",
    "                \"precipitation\": row_cleaned.get(\"prcp\"),\n",
    "                \"snow\": row_cleaned.get(\"snow\"),\n",
    "                \"wind_speed\": row_cleaned.get(\"wspd\"),\n",
    "                \"humidity\": row_cleaned.get(\"rhum\"),\n",
    "            }\n",
    "            with open(path, \"w\") as f:\n",
    "                json.dump(weather_info, f, indent=2)\n",
    "            print(f\" Donn√©es sauvegard√©es dans {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\" Erreur : {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac16681",
   "metadata": {},
   "source": [
    "## 4. Nettoyage des fichiers JSON bruts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ebe192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_write(source_folder=data_brut_path, destination_folder=data_propre_path):\n",
    "    os.makedirs(destination_folder, exist_ok=True)\n",
    "    for filename in os.listdir(source_folder):\n",
    "        if filename.endswith('.json'):\n",
    "            source_path = os.path.join(source_folder, filename)\n",
    "            destination_path = os.path.join(destination_folder, filename)\n",
    "            with open(source_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            cleaned_data = {\n",
    "                key: (0.0 if value is None else value)\n",
    "                for key, value in data.items()\n",
    "            }\n",
    "            with open(destination_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(cleaned_data, f, ensure_ascii=False, indent=2)\n",
    "    print(\" Nettoyage termin√©.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3cf0ff",
   "metadata": {},
   "source": [
    "## 5. Fusion des fichiers JSON nettoy√©s en un CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37212e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all_json_to_csv(source_folder=data_propre_path, output_path=data_final_csv):\n",
    "    records = []\n",
    "    for filename in os.listdir(source_folder):\n",
    "        if filename.endswith('.json'):\n",
    "            file_path = os.path.join(source_folder, filename)\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "                if 'date' not in data:\n",
    "                    continue\n",
    "                data['date'] = pd.to_datetime(data['date'])\n",
    "                for key in ['temperature_avg', 'temperature_min', 'temperature_max', 'precipitation', 'snow', 'wind_speed', 'humidity']:\n",
    "                    data[key] = data.get(key, 0.0) or 0.0\n",
    "                records.append(data)\n",
    "            except Exception as e:\n",
    "                print(f\"Erreur dans {filename} : {e}\")\n",
    "    if not records:\n",
    "        print(\"Aucune donn√©e trouv√©e.\")\n",
    "        return\n",
    "    df = pd.DataFrame(records)\n",
    "    df['date'] = df['date'].dt.date\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "    print(f\" Fichier fusionn√© enregistr√© ici : {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592b597d",
   "metadata": {},
   "source": [
    "## 6. Upload du CSV vers Google Sheets via Drive API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8dc03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_sheet(service, sheet_name, folder_id):\n",
    "    query = f\"'{folder_id}' in parents and name = '{sheet_name}' and mimeType = 'application/vnd.google-apps.spreadsheet' and trashed = false\"\n",
    "    results = service.files().list(q=query, fields=\"files(id)\").execute()\n",
    "    files = results.get(\"files\", [])\n",
    "    if files:\n",
    "        print(f\" Feuille trouv√©e : {files[0]['id']}\")\n",
    "        return files[0]['id']\n",
    "    else:\n",
    "        file_metadata = {\n",
    "            \"name\": sheet_name,\n",
    "            \"mimeType\": \"application/vnd.google-apps.spreadsheet\",\n",
    "            \"parents\": [folder_id]\n",
    "        }\n",
    "        file = service.files().create(body=file_metadata, fields=\"id\").execute()\n",
    "        print(f\" Nouvelle feuille cr√©√©e : {file['id']}\")\n",
    "        return file['id']\n",
    "\n",
    "def write_df_to_sheet(service, spreadsheet_id, df, sheet_name='Sheet1'):\n",
    "    sheets_service = service.spreadsheets()\n",
    "    sheets_service.values().clear(spreadsheetId=spreadsheet_id, range=sheet_name).execute()\n",
    "    values = [df.columns.tolist()] + df.values.tolist()\n",
    "    sheets_service.values().update(\n",
    "        spreadsheetId=spreadsheet_id,\n",
    "        range=sheet_name,\n",
    "        valueInputOption=\"RAW\",\n",
    "        body={\"values\": values}\n",
    "    ).execute()\n",
    "    print(f\"{len(df)} lignes √©crites.\")\n",
    "\n",
    "def upload_csv_to_drive(service_account_path, file_path, file_name, folder_id):\n",
    "    creds = service_account.Credentials.from_service_account_file(\n",
    "        service_account_path,\n",
    "        scopes=['https://www.googleapis.com/auth/drive', 'https://www.googleapis.com/auth/spreadsheets']\n",
    "    )\n",
    "    drive_service = build('drive', 'v3', credentials=creds)\n",
    "    sheets_service = build('sheets', 'v4', credentials=creds)\n",
    "    df = pd.read_csv(file_path)\n",
    "    sheet_name = os.path.splitext(file_name)[0]\n",
    "    spreadsheet_id = get_or_create_sheet(drive_service, sheet_name, folder_id)\n",
    "    write_df_to_sheet(sheets_service, spreadsheet_id, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b5e3c1",
   "metadata": {},
   "source": [
    "## 7.  Ex√©cution manuelle √©tape par √©tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca8b0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch_and_save_history('2020-01-01')\n",
    "# clean_and_write()\n",
    "# merge_all_json_to_csv()\n",
    "# upload_csv_to_drive('service_account.json', data_final_csv, 'cape_town.csv', 'ID_DOSSIER_GOOGLE_DRIVE')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
